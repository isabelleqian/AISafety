{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabelleqian/AISafety/blob/main/Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYHEP9tPocQj"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnVkRLynoYhT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "chapter = \"chapter3_llm_evals\"\n",
        "repo = \"ARENA_3.0\"\n",
        "branch = \"main\"\n",
        "\n",
        "# Install dependencies\n",
        "try:\n",
        "    import inspect_ai\n",
        "except:\n",
        "    %pip install openai>=1.58.1 anthropic inspect_ai tabulate wikipedia jaxtyping python-dotenv datasets\n",
        "\n",
        "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "\n",
        "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
        "    if not IN_COLAB:\n",
        "        !sudo apt-get install unzip\n",
        "        %pip install jupyter ipython --upgrade\n",
        "\n",
        "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
        "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
        "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
        "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
        "        !rm {root}/{branch}.zip\n",
        "        !rmdir {root}/{repo}-{branch}\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import output, userdata\n",
        "\n",
        "    for key in [\"OPENAI\", \"ANTHROPIC\"]:\n",
        "        try:\n",
        "            os.environ[f\"{key}_API_KEY\"] = userdata.get(f\"{key}_API_KEY\")\n",
        "        except:\n",
        "            warnings.warn(\n",
        "                f\"You don't have a '{key}_API_KEY' variable set in the secrets tab of your google colab. You have to set one, or calls to the {key} API won't work.\"\n",
        "            )\n",
        "\n",
        "# Handles running code in an ipynb\n",
        "if \"__file__\" not in globals() and \"__vsc_ipynb_file__\" in globals():\n",
        "    __file__ = globals()[\"__vsc_ipynb_file__\"]\n",
        "\n",
        "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
        "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "os.chdir(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from typing import Any, Literal\n",
        "\n",
        "from anthropic import Anthropic\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = \"chapter3_llm_evals\"\n",
        "section = \"part3_running_evals_with_inspect\"\n",
        "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
        "exercises_dir = root_dir / chapter / \"exercises\"\n",
        "section_dir = exercises_dir / section\n",
        "if str(exercises_dir) not in sys.path:\n",
        "    sys.path.append(str(exercises_dir))\n",
        "\n",
        "import part3_running_evals_with_inspect.tests as tests\n",
        "\n",
        "MAIN = __name__ == \"__main__\"\n",
        "\n",
        "\n",
        "# OPENAI_API_KEY and ANTHROPIC_API_KEY\n",
        "\n",
        "openai_client = OpenAI()\n",
        "anthropic_client = Anthropic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrvqiXlfoqMW"
      },
      "source": [
        "# **Load Arc Challenge Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWTktOakosh2",
        "outputId": "e91859ea-a905-4226-b20e-87b59e876fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "%pip install --upgrade datasets\n",
        "\n",
        "model_id = \"ft:gpt-4.1-nano-2025-04-14:algoverse:arc-100-v1:BiRcF3ec\"\n",
        "temperature = 0\n",
        "dataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16f00b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "1d016a44-63f8-423f-8b46-3b03141d0d08"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Dataset() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3833323096.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Create the inspect_ai Dataset object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0marc_dataset_inspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marc_samples_adapted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass the list of samples directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Pretty-print the first converted sample to verify the structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dataset() takes no arguments"
          ]
        }
      ],
      "source": [
        "from inspect_ai.dataset import Sample, Dataset, hf_dataset\n",
        "from inspect_ai.model import ChatMessageUser, ChatMessageSystem\n",
        "\n",
        "# Define the function to map dataset records to Sample objects\n",
        "def record_to_sample_arc_adapted(record: dict) -> Sample:\n",
        "    \"\"\"\n",
        "    Maps a record from the ARC Challenge dataset to a Sample object, including choices.\n",
        "    \"\"\"\n",
        "    question = record['question']\n",
        "    choices = record['choices']\n",
        "    answer_key = record['answerKey']\n",
        "\n",
        "    # Format choices for the input string\n",
        "    choice_text_list = []\n",
        "    for i in range(len(choices['text'])):\n",
        "        choice_text_list.append(f\"{choices['label'][i]}. {choices['text'][i]}\")\n",
        "    choice_text = \"\\n\".join(choice_text_list)\n",
        "\n",
        "    # Create the input string including the question and formatted choices\n",
        "    input_text = f\"{question}\\n\\n{choice_text}\"\n",
        "    input = [ChatMessageUser(content=input_text)] # Use ChatMessageUser for input\n",
        "\n",
        "    # Extract choices as a list of strings (for the 'choices' field in Sample)\n",
        "    choice_list = [f\"{choices['label'][i]}. {choices['text'][i]}\" for i in range(len(choices['text']))]\n",
        "\n",
        "    # Find the target based on the answerKey (the text of the correct answer)\n",
        "    target = None\n",
        "    for i, label in enumerate(choices['label']):\n",
        "        if label == answer_key:\n",
        "            target = choices['text'][i]\n",
        "            break\n",
        "\n",
        "    # Optional: add metadata\n",
        "    metadata = {\"original_answerKey\": answer_key, \"original_choices\": choices}\n",
        "\n",
        "    return Sample(\n",
        "        input=input,\n",
        "        target=target,\n",
        "        choices=choice_list, # Include the choices as a list\n",
        "        metadata=metadata,\n",
        "    )\n",
        "\n",
        "# Load the ARC Challenge dataset using hf_dataset and the mapping function\n",
        "arc_dataset_inspect = hf_dataset(\n",
        "    path=\"allenai/ai2_arc\",\n",
        "    name=\"ARC-Challenge\",\n",
        "    sample_fields=record_to_sample_arc_adapted,\n",
        "    split=\"train\", # Use the train split as in the original attempt\n",
        "    trust=True,\n",
        ")\n",
        "\n",
        "# Pretty-print the first converted sample to verify the structure\n",
        "print(\"--- First converted sample ---\")\n",
        "pprint(arc_dataset_inspect.samples[0].__dict__)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "from inspect_ai import Task, eval, task\n",
        "from inspect_ai.scorer import match, model_graded_fact\n",
        "from inspect_ai.solver import chain_of_thought, generate, self_critique\n",
        "\n",
        "@task\n",
        "def arc_challenge_task() -> Task:\n",
        "    # Return the Task using the created Dataset object\n",
        "    return Task(\n",
        "        dataset=arc_dataset_inspect,\n",
        "        solver=[chain_of_thought(), generate(), self_critique(model=\"openai/gpt-4o-mini\")],\n",
        "        scorer=model_graded_fact(model=\"openai/gpt-4o-mini\"),\n",
        "    )\n",
        "\n",
        "# Now call eval with the task function name\n",
        "# The @task decorator handles turning the function into a Task object\n",
        "log = eval(arc_challenge_task, model=\"openai/gpt-4o-mini\", limit=10, log_dir=str(section_dir / \"logs\"))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c4d46f6",
        "outputId": "2b7f80ca-7650-46ee-8fdb-e5dce4ead343"
      },
      "source": [
        "# Install or upgrade necessary libraries if not already done\n",
        "%pip install --upgrade datasets inspect_ai openai anthropic tabulate wikipedia jaxtyping python-dotenv\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from datasets import load_dataset\n",
        "from inspect_ai import Task, eval, task\n",
        "from inspect_ai.dataset import Dataset, Sample\n",
        "from inspect_ai.scorer import model_graded_fact\n",
        "from inspect_ai.solver import chain_of_thought, generate, self_critique\n",
        "from inspect_ai.model import ChatMessageUser, ChatMessageSystem\n",
        "from typing import Any\n",
        "\n",
        "# Setup paths and environment variables (similar to your initial setup)\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "chapter = \"chapter3_llm_evals\"\n",
        "repo = \"ARENA_3.0\"\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "section = \"part3_running_evals_with_inspect\"\n",
        "section_dir = Path(root) / chapter / \"exercises\" / section # Assuming this path structure\n",
        "\n",
        "# Load API keys if in Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    for key in [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]:\n",
        "        try:\n",
        "            os.environ[key] = userdata.get(key)\n",
        "        except:\n",
        "            print(f\"Warning: '{key}' not found in Colab secrets.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: inspect_ai in /usr/local/lib/python3.11/dist-packages (0.3.114)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.0)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.57.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (3.11.15)\n",
            "Requirement already satisfied: anyio>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (4.13.4)\n",
            "Requirement already satisfied: click<8.2.0,>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (8.1.8)\n",
            "Requirement already satisfied: debugpy in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (1.8.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (0.16)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (0.28.1)\n",
            "Requirement already satisfied: ijson>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (3.4.0)\n",
            "Requirement already satisfied: jsonlines>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (4.0.0)\n",
            "Requirement already satisfied: jsonpatch>=1.32 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (1.33)\n",
            "Requirement already satisfied: jsonpath-ng>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (1.7.0)\n",
            "Requirement already satisfied: jsonref>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (1.1.0)\n",
            "Requirement already satisfied: jsonschema>3.1.1 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (4.24.0)\n",
            "Requirement already satisfied: mmh3>3.1.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (5.1.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (1.6.0)\n",
            "Requirement already satisfied: platformdirs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (4.3.8)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.11.4 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (2.11.7)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.3.3 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (13.9.4)\n",
            "Requirement already satisfied: s3fs>=2023 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (2025.3.0)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (3.0.4)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (1.0.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (1.3.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (8.5.0)\n",
            "Requirement already satisfied: textual<v3.0.0,>=0.86.2 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (2.1.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (4.14.1)\n",
            "Requirement already satisfied: zipp>=3.19.1 in /usr/local/lib/python3.11/dist-packages (from inspect_ai) (3.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping) (0.1.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->inspect_ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->inspect_ai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->inspect_ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->inspect_ai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->inspect_ai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->inspect_ai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->inspect_ai) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.8.0->inspect_ai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->inspect_ai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->inspect_ai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->inspect_ai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch>=1.32->inspect_ai) (3.0.0)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.11/dist-packages (from jsonpath-ng>=1.7.0->inspect_ai) (3.11)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>3.1.1->inspect_ai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>3.1.1->inspect_ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>3.1.1->inspect_ai) (0.26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.4->inspect_ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.4->inspect_ai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.4->inspect_ai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.3.3->inspect_ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.3.3->inspect_ai) (2.19.2)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from s3fs>=2023->inspect_ai) (2.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->inspect_ai) (2.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023->inspect_ai) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.38.28,>=1.38.23 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023->inspect_ai) (1.38.27)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023->inspect_ai) (1.0.1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023->inspect_ai) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.3.3->inspect_ai) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual<v3.0.0,>=0.86.2->inspect_ai) (2.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual<v3.0.0,>=0.86.2->inspect_ai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual<v3.0.0,>=0.86.2->inspect_ai) (1.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "0666386f",
        "outputId": "4b4c83ad-5032-4c50-867b-18c52ae92936"
      },
      "source": [
        "# Define the function to map dataset records to Sample objects\n",
        "def record_to_sample_arc_adapted(record: dict) -> Sample:\n",
        "    \"\"\"\n",
        "    Maps a record from the ARC Challenge dataset to a Sample object, including choices.\n",
        "    \"\"\"\n",
        "    question = record['question']\n",
        "    choices = record['choices']\n",
        "    answer_key = record['answerKey']\n",
        "\n",
        "    # Format choices for the input string\n",
        "    choice_text_list = []\n",
        "    for i in range(len(choices['text'])):\n",
        "        choice_text_list.append(f\"{choices['label'][i]}. {choices['text'][i]}\")\n",
        "    choice_text = \"\\n\".join(choice_text_list)\n",
        "\n",
        "    # Create the input string including the question and formatted choices\n",
        "    input_text = f\"{question}\\n\\n{choice_text}\"\n",
        "    input = [ChatMessageUser(content=input_text)] # Use ChatMessageUser for input\n",
        "\n",
        "    # Extract choices as a list of strings (for the 'choices' field in Sample)\n",
        "    choice_list = [f\"{choices['label'][i]}. {choices['text'][i]}\" for i in range(len(choices['text']))]\n",
        "\n",
        "    # Find the target based on the answerKey (the text of the correct answer)\n",
        "    target = None\n",
        "    for i, label in enumerate(choices['label']):\n",
        "        if label == answer_key:\n",
        "            target = choices['text'][i]\n",
        "            break\n",
        "\n",
        "    # Optional: add metadata\n",
        "    metadata = {\"original_answerKey\": answer_key, \"original_choices\": choices}\n",
        "\n",
        "    return Sample(\n",
        "        input=input,\n",
        "        target=target,\n",
        "        choices=choice_list, # Include the choices as a list\n",
        "        metadata=metadata,\n",
        "    )\n",
        "\n",
        "# Load the ARC Challenge dataset\n",
        "dataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")\n",
        "\n",
        "# Process the 'train' split of the dataset using record_to_sample_arc_adapted\n",
        "# Convert the 'train' split to a list of dictionaries before processing for robustness\n",
        "train_records = list(dataset['train'])\n",
        "arc_samples_adapted = [record_to_sample_arc_adapted(record) for record in train_records]\n",
        "\n",
        "# Create the inspect_ai Dataset object\n",
        "arc_dataset_inspect = Dataset.from_samples(arc_samples_adapted) # Use from_samples\n",
        "\n",
        "# Pretty-print the first converted sample to verify the structure\n",
        "print(\"--- First converted sample ---\")\n",
        "pprint(arc_samples_adapted[0].__dict__)\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Dataset() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-671648076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Create the inspect_ai Dataset object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0marc_dataset_inspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marc_samples_adapted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Pretty-print the first converted sample to verify the structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dataset() takes no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "46addf61",
        "outputId": "5b159502-432b-45e3-d99a-1af17c9076a5"
      },
      "source": [
        "# Define the inspect_ai Task\n",
        "@task\n",
        "def arc_challenge_task() -> Task:\n",
        "    \"\"\"\n",
        "    InspectAI Task for the ARC Challenge dataset.\n",
        "    \"\"\"\n",
        "    # Use the created Dataset object\n",
        "    return Task(\n",
        "        dataset=arc_dataset_inspect, # Use the Dataset created in the previous cell\n",
        "        solver=[chain_of_thought(), generate(), self_critique(model=\"openai/gpt-4o-mini\")],\n",
        "        scorer=model_graded_fact(model=\"openai/gpt-4o-mini\"),\n",
        "    )\n",
        "\n",
        "# Run the evaluation using inspect_ai\n",
        "# Adjust the 'limit' parameter to control the number of samples evaluated\n",
        "# Ensure the log_dir exists or is handled\n",
        "log_dir_path = section_dir / \"logs\"\n",
        "log_dir_path.mkdir(parents=True, exist_ok=True) # Create log directory if it doesn't exist\n",
        "\n",
        "print(f\"Running evaluation with inspect_ai on {len(arc_samples_adapted)} samples (limit={10})...\")\n",
        "log = eval(arc_challenge_task, model=\"openai/gpt-4o-mini\", limit=10, log_dir=str(log_dir_path))\n",
        "\n",
        "# You can then inspect the 'log' object to see the evaluation results\n",
        "print(\"\\nEvaluation completed. Log object available.\")\n",
        "# pprint(log) # Uncomment to print the full log object"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'task' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-969165050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the inspect_ai Task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marc_challenge_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mInspectAI\u001b[0m \u001b[0mTask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mARC\u001b[0m \u001b[0mChallenge\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'task' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "chapter = \"chapter3_llm_evals\"\n",
        "repo = \"ARENA_3.0\"\n",
        "branch = \"main\"\n",
        "\n",
        "# Install dependencies\n",
        "try:\n",
        "    import inspect_ai\n",
        "except:\n",
        "    %pip install openai>=1.58.1 anthropic inspect_ai tabulate wikipedia jaxtyping python-dotenv datasets\n",
        "\n",
        "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "\n",
        "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
        "    if not IN_COLAB:\n",
        "        !sudo apt-get install unzip\n",
        "        %pip install jupyter ipython --upgrade\n",
        "\n",
        "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
        "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
        "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
        "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
        "        !rm {root}/{branch}.zip\n",
        "        !rmdir {root}/{repo}-{branch}\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import output, userdata\n",
        "\n",
        "    for key in [\"OPENAI\", \"ANTHROPIC\"]:\n",
        "        try:\n",
        "            os.environ[f\"{key}_API_KEY\"] = userdata.get(f\"{key}_API_KEY\")\n",
        "        except:\n",
        "            warnings.warn(\n",
        "                f\"You don't have a '{key}_API_KEY' variable set in the secrets tab of your google colab. You have to set one, or calls to the {key} API won't work.\"\n",
        "            )\n",
        "\n",
        "# Handles running code in an ipynb\n",
        "if \"__file__\" not in globals() and \"__vsc_ipynb_file__\" in globals():\n",
        "    __file__ = globals()[\"__vsc_ipynb_file__\"]\n",
        "\n",
        "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
        "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "os.chdir(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from typing import Any, Literal\n",
        "\n",
        "from anthropic import Anthropic\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = \"chapter3_llm_evals\"\n",
        "section = \"part3_running_evals_with_inspect\"\n",
        "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
        "exercises_dir = root_dir / chapter / \"exercises\"\n",
        "section_dir = exercises_dir / section\n",
        "if str(exercises_dir) not in sys.path:\n",
        "    sys.path.append(str(exercises_dir))\n",
        "\n",
        "import part3_running_evals_with_inspect.tests as tests\n",
        "\n",
        "MAIN = __name__ == \"__main__\"\n",
        "\n",
        "\n",
        "# OPENAI_API_KEY and ANTHROPIC_API_KEY\n",
        "\n",
        "openai_client = OpenAI()\n",
        "anthropic_client = Anthropic()\n",
        "\n",
        "from datasets import load_dataset\n",
        "%pip install --upgrade datasets\n",
        "\n",
        "model_id = \"ft:gpt-4.1-nano-2025-04-14:algoverse:arc-100-v1:BiRcF3ec\"\n",
        "temperature = 0\n",
        "dataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")\n",
        "\n",
        "from inspect_ai.dataset import Sample, hf_dataset\n",
        "from inspect_ai.model import ChatMessageAssistant, ChatMessageSystem, ChatMessageUser\n",
        "%pip install --upgrade datasets\n",
        "\n",
        "def arc_record_to_sample(record: dict[str, Any]) -> Sample:\n",
        "    \"\"\"\n",
        "    Formats dataset records which look like this:\n",
        "        {\n",
        "            \"answerKey\": \"B\",\n",
        "            \"choices\": {\n",
        "                \"label\": [\"A\", \"B\", \"C\", \"D\"],\n",
        "                \"text\": [\"Shady areas increased.\", \"Food sources increased.\", ...]\n",
        "            },\n",
        "            \"question\": \"...Which best explains why there were more chipmunks the next year?\"\n",
        "        }\n",
        "    \"\"\"\n",
        "    labels = record[\"choices\"][\"label\"]\n",
        "    choices = record[\"choices\"][\"text\"]\n",
        "\n",
        "    target = chr(ord(\"A\") + labels.index(record[\"answerKey\"]))  # maps target label to A, B, C, ...\n",
        "    input = [ChatMessageUser(content=record[\"question\"])]  # should store input as list of ChatMessage objects\n",
        "\n",
        "    # return sample\n",
        "    return Sample(input=input, choices=choices, target=target)\n",
        "\n",
        "\n",
        "dataset = hf_dataset(\n",
        "    path=\"allenai/ai2_arc\",\n",
        "    name=\"ARC-Challenge\",\n",
        "    sample_fields=arc_record_to_sample,\n",
        "    split=\"validation\",\n",
        "    trust=True,\n",
        ")\n",
        "pprint(dataset.samples[0].__dict__)\n",
        "\n",
        "from inspect_ai.dataset import json_dataset\n",
        "\n",
        "\n",
        "def record_to_sample(record: dict) -> Sample:\n",
        "    \"\"\"\n",
        "    Converts a item (\"record\") from the dataset into a Sample object, mapping the fields of the record to the fields of\n",
        "    the Sample object.\n",
        "\n",
        "    Args:\n",
        "        record : A dictionary from the json dataset containing our evaluation questions\n",
        "\n",
        "    Returns:\n",
        "        Sample : A Sample object containing the information in the record\n",
        "    \"\"\"\n",
        "    input = [ChatMessageUser(content=record[\"question\"])]\n",
        "    with_system_prompt = record.get(\"system\", \"\") != \"\"\n",
        "    if with_system_prompt:\n",
        "        input.insert(0, ChatMessageSystem(content=record[\"system\"]))\n",
        "\n",
        "    return Sample(\n",
        "         input=input,\n",
        "        target=record[\"answer_matching_behavior\"],\n",
        "        choices=list(record[\"answers\"].values()),\n",
        "        metadata={\"labels\": list(record[\"answers\"].keys()), \"behavior_category\": record[\"behavior_category\"], \"system_prompt\": with_system_prompt,},\n",
        "    )\n",
        "\n",
        "\n",
        "# Edit these variables depending on what you saved yesterday!\n",
        "evaluation_target = \"power-seeking\"\n",
        "num_qs_saved = 300\n",
        "\n",
        "json_dataset_path = str(exercises_dir / \"part2_dataset_generation\" / f\"{evaluation_target}_{num_qs_saved}_qs.json\")\n",
        "my_dataset = json_dataset(json_dataset_path, record_to_sample)\n",
        "\n",
        "# Pretty-print the data in the Samples object, so we can see its structure\n",
        "pprint(my_dataset.samples[0].__dict__)\n",
        "\n",
        "from inspect_ai.solver import Generate, Solver, TaskState, chain, solver\n",
        "\n",
        "\n",
        "@solver\n",
        "def system_message(system_message: str) -> Solver:\n",
        "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
        "        last_system_message_idx = max(\n",
        "            [-1] + [i for i, msg in enumerate(state.messages) if isinstance(msg, ChatMessageSystem)]\n",
        "        )\n",
        "        state.messages.insert(last_system_message_idx + 1, ChatMessageSystem(content=system_message))\n",
        "        return state\n",
        "\n",
        "    return solve\n",
        "\n",
        "\n",
        "from inspect_ai.dataset import Dataset\n",
        "from inspect_ai.scorer import Scorer\n",
        "\n",
        "\n",
        "@solver\n",
        "def prompt_template(template: str) -> Solver:\n",
        "    \"\"\"\n",
        "    Returns a solve function which modifies the user prompt with the given template.\n",
        "\n",
        "    Args:\n",
        "        template : The template string to use to modify the user prompt. Must include {prompt} to be replaced with the original user prompt.\n",
        "\n",
        "    Returns:\n",
        "        solve : A solve function which modifies the user prompt with the given template\n",
        "    \"\"\"\n",
        "    # Check {prompt} is in the template, but no other fields\n",
        "    assert set(re.findall(r\"\\{.*?\\}\", template)) == {r\"{prompt}\"}, r\"Template must include {prompt} field and no others\"\n",
        "\n",
        "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
        "        # YOUR CODE HERE - implement the prompt_template solver\n",
        "        state.user_prompt.text = template.format(prompt=state.user_prompt.text)\n",
        "\n",
        "        return state\n",
        "\n",
        "    return solve\n",
        "\n",
        "\n",
        "def test_my_solver(solver: Solver, dataset: Dataset, n: int = 5, scorer: Scorer = match()):\n",
        "    \"\"\"\n",
        "    Helper function which will test your solver on a dataset of `n` examples. The logs are saved to `test_logs/`, and\n",
        "    can be viewed using the `inspect view` command (or the VS Code Inspect extension).\n",
        "    \"\"\"\n",
        "\n",
        "    @task\n",
        "    def test_task() -> Task:\n",
        "        return Task(dataset=dataset, solver=solver, scorer=scorer)\n",
        "\n",
        "    log = eval(test_task(), model=\"openai/gpt-4o-mini\", limit=n, log_dir=str(section_dir / \"test_logs\"))\n",
        "    return log\n",
        "\n",
        "\n",
        "my_solver = chain(\n",
        "    prompt_template(template=\"{prompt}\\n\\nAnswer in the form of a limerick.\"),\n",
        "    generate(),\n",
        ")\n",
        "log = test_my_solver(my_solver, my_dataset)\n",
        "\n",
        "\n",
        "from inspect_ai.scorer import answer\n",
        "from inspect_ai.solver import Choices\n",
        "\n",
        "\n",
        "def letters_and_answer_options(choices: Choices) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Helper function, returns `choices` formatted as MCQ options, as well as the string of labels for each option.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        [\"choice 1\", \"choice 2\", \"choice 3\"] -> (\n",
        "            \"A) choice 1\\nB) choice 2\\nC) choice 3\",\n",
        "            \"A, B, C\"\n",
        "        )\n",
        "    \"\"\"\n",
        "    letters = [chr(65 + i) for i in range(len(choices))]\n",
        "\n",
        "    return (\n",
        "        \", \".join(letters),\n",
        "        \"\\n\".join([f\"{letter}) {choice.value}\" for letter, choice in zip(letters, choices)]),\n",
        "    )\n",
        "\n",
        "\n",
        "@solver\n",
        "def multiple_choice_format(template: str = TEMPLATE_MCQ) -> Solver:\n",
        "    \"\"\"\n",
        "    Returns a solve function which modifies the initial prompt to be in the format of an MCQ.\n",
        "\n",
        "    Args:\n",
        "        template: The template string to use to modify the user prompt. Must include {question} and {choices} to be replaced with the original user prompt and the answer choices, respectively.\n",
        "\n",
        "    Returns:\n",
        "        solve: A solve function which modifies the user prompt with the given template\n",
        "    \"\"\"\n",
        "    tags = set(re.findall(r\"\\{.*?\\}\", template))\n",
        "    assert r\"{question}\" in tags, \"Template must include {question} field\"\n",
        "    assert r\"{choices}\" in tags, \"Template must include {choices} field\"\n",
        "    assert tags - {r\"{question}\", r\"{choices}\", r\"{letters}\"} == set(), \"Unexpected field found in template\"\n",
        "\n",
        "    async def solve(state: TaskState, generate: Generate) -> TaskState:\n",
        "        assert state.choices, \"If using MCQ then state must have `choices` field\"\n",
        "        # YOUR CODE HERE - implement the multiple_choice_format solver\n",
        "        letters, choices = letters_and_answer_options(state.choices)\n",
        "        state.user_prompt.text = template.format(question=state.user_prompt.text, choices=choices, letters=letters)\n",
        "\n",
        "        return state\n",
        "\n",
        "    return solve\n",
        "\n",
        "\n",
        "my_solver = chain(\n",
        "    multiple_choice_format(template=TEMPLATE_MCQ),\n",
        "    generate(),\n",
        ")\n",
        "log = test_my_solver(my_solver, my_dataset, scorer=answer(\"letter\"))\n",
        "\n",
        "# Check the sample output is in the correct format, and was parsed correctly\n",
        "assert log[0].samples[0].scores[\"answer\"].answer in [\"A\", \"B\"]\n",
        "assert log[0].samples[0].scores[\"answer\"].explanation in [\"ANSWER: A\", \"ANSWER: B\"]\n",
        "\n",
        "from inspect_ai.dataset import example_dataset\n",
        "\n",
        "dataset = example_dataset(\"theory_of_mind\")\n",
        "pprint(dataset.samples[0].__dict__)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9d2IV3l62PQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmXqk4bHlBBrCxUAt5Vuji",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}